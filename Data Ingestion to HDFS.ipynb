{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestión de datos de películas en HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de este notebook es escribir los datos de películas en el DataLake de Hadoop. Para esto es necesario que en el mismo directorio que esta notebook se encuentre una carpeta con los archivos .csv de los datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación del framework spark, por si no esta instalado aun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la sesión de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Movie data ingestion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de archivos .csv de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_movies = \"movies_metadata\"\n",
    "name_ratings = \"ratings\"\n",
    "name_credits = \"credits\"\n",
    "name_links = \"links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_movies}.csv')\n",
    "ratingsDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_ratings}.csv')\n",
    "linksDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_links}.csv')\n",
    "creditsDF= spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_credits}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escritura de los datos en HDFS, con formato columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_dest_folder_parquet = 'hdfs://192.168.56.101:9000/obligatorio/datasets_parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_movies}')\n",
    "ratingsDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_ratings}')\n",
    "linksDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_links}')\n",
    "creditsDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_credits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_dest_folder_csv = 'hdfs://192.168.56.101:9000/obligatorio/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_movies}')\n",
    "ratingsDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_ratings}')\n",
    "linksDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_links}')\n",
    "creditsDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_credits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
