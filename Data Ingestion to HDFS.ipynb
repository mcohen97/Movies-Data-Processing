{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestión de datos de películas en HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de este notebook es escribir los datos de películas en el DataLake de Hadoop. Para esto es necesario que en el mismo directorio que esta notebook se encuentre una carpeta con los archivos .csv de los datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación del framework spark, por si no esta instalado aun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/marcel/anaconda3/lib/python3.7/site-packages (2.4.5)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/marcel/anaconda3/lib/python3.7/site-packages (from pyspark) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de la sesión de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Movie data ingestion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de archivos .csv de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_movies = \"movies_metadata\"\n",
    "name_ratings = \"ratings\"\n",
    "name_credits = \"credits\"\n",
    "name_links = \"links\"\n",
    "name_keywords = \"keywords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_movies}.csv')\n",
    "ratingsDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_ratings}.csv')\n",
    "linksDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_links}.csv')\n",
    "creditsDF= spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_credits}.csv')\n",
    "keywordsDF= spark.read.format(\"csv\").option(\"header\", \"true\").load(f'datasets/{name_keywords}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escritura de los datos en HDFS, con formato columnar Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_dest_folder_parquet = 'hdfs://192.168.56.101:9000/obligatorio/datasets_parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_movies}')\n",
    "ratingsDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_ratings}')\n",
    "linksDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_links}')\n",
    "creditsDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_credits}')\n",
    "keywordsDF.write.mode('overwrite').parquet(f'{hadoop_dest_folder_parquet}/{name_keywords}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escritura de los datos en HDFS, en el formato CSV original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_dest_folder_csv = 'hdfs://192.168.56.101:9000/obligatorio/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keywordsDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3b9c3a1c9c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlinksDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{hadoop_dest_folder_csv}/{name_links}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcreditsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{hadoop_dest_folder_csv}/{name_credits}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mkeywordsDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{hadoop_dest_folder_csv}/{name_keywords}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'keywordsDF' is not defined"
     ]
    }
   ],
   "source": [
    "moviesDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_movies}')\n",
    "ratingsDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_ratings}')\n",
    "linksDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_links}')\n",
    "creditsDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_credits}')\n",
    "keywordsDF.write.mode('overwrite').csv(f'{hadoop_dest_folder_csv}/{name_keywords}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cierre de la sesión Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
